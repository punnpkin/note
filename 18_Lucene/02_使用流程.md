# 二、使用流程

索引的创建一般分为以下几步：

第一步：将文档交给切词组件(Tokenizer)进行**切词处理**（切分词）

第二步：将得到的词元(Token)交给**语言处理组件**（大小写，单复数，分词等等）

第三步：将得到的词(Term)交给**索引组件**

第四步：由索引组件**针对词建立索引**

## 1. 数据采集

## 2. 构建文档和域

```java
Document document = new Document();
document.add(new StringField("id", sku.getId(), Field.Store.YES));
```

域的属性有很多：

* 是否分词：分词的目的是为了索引。

  商品名称，商品分类等需要分词，id，订单号等不分词。

* 是否索引：进行索引。将Field分词后的词或整个Field值进行索引，存储到索引域，索引的目的是为了搜索。

* 是否存储：将Field值存储在文档域中，存储在文档域中的Field才可以从Document中获取。

  商品名称、订单号，凡是将来要从Document中获取的Field都要存储。

  商品描述，内容较大不用存储。如果要向用户展示商品描述可以从系统的关系数据库中获取。

## 3. 初始化分词器

在对Document中的内容进行索引之前，需要使用分词器进行分词 ，分词的目的是为了搜索。分词的主要过程就是先分词后过滤。

> 什么是停用词？停用词是为节省存储空间和提高搜索效率，搜索引擎在索引页面或处理搜索请求时会自动忽略某些字或词，这些字或词即被称为Stop Words(停用词)。比如语气助词、副词、介词、连接词等，通常自身并无明确的意义，只 有将其放入一个完整的句子中才有一定作用，如常见的“的”、“在”、“是”、“啊”等。

### 3.1 分词器使用的时机

#### 索引时使用Analyzer

输入关键字进行搜索，当需要让该关键字与文档域内容所包含的词进行匹配时需要对文档域内容进行分析，需要经过Analyzer分析器处理生成**语汇单元（Token）**。

分析器分析的对象是文档中的**Field域**。当Field的属性tokenized（是否分词）为true时会对Field值进行分析。

#### 搜索时使用Analyzer

对搜索关键字进行分析和索引分析一样，使用Analyzer对搜索关键字进行分析、分词处理，使用分析后每个词语进行搜索。比如：搜索关键字：spring web ，经过分析器进行分词，得出：spring web拿词去索引词典表查找 ，找到索引链接到Document，解析Document内容。
对于匹配整体Field域的查询可以在搜索时不分析，比如根据订单号、身份证号查询等。

### 3.2 分词器

#### StandardAnalyzer

Lucene提供的标准分词器, 可以对用英文进行分词, 对中文是单字分词, 也就是一个字就认为是一个词。

```java
protected TokenStreamComponents createComponents(String fieldName) {
    final StandardTokenizer src = new StandardTokenizer();
    src.setMaxTokenLength(this.maxTokenLength);
    TokenStream tok = new LowerCaseFilter(src);
    TokenStream tok = new StopFilter(tok, this.stopwords);
    return new TokenStreamComponents(src, tok) {
            protected void setReader(Reader reader) {
            src.setMaxTokenLength(StandardAnalyzer.this.maxTokenLength);
            super.setReader(reader);
        }
    };
}
```



Tokenizer就是分词器，负责将reader转换为语汇单元即进行分词处理，Lucene提供了很多的分词器，也可以使用第三方的分词，比如IKAnalyzer一个中文分词器。

TokenFilter是分词过滤器，负责对语汇单元进行过滤，TokenFilter可以是一个过滤器链儿，Lucene提供了很多的分词器过滤器，比如大小写转换、去除停用词等。

语汇单元的生成过程：

Reader->Tokenizer(分词)->TokenFilter(标准过滤)->TokenFilter(大小写过滤)->TokenFilter(停用词过滤)->Tokens

#### IKAnalyzer

适用于中文的分词器

##### 停用词典stopword.dic作用 :

停用词典中的词例如: a, an, the, 的, 地, 得等词汇, 凡是出现在停用词典中的字或者词, 在切分词的时候会被过滤掉.

##### 扩展词典ext.dic作用 :

扩展词典中的词例如: 传智播客，黑马程序员，贵州茅台等专有名词，汉语中一些公司名称，行业名称，分类，品牌等不是汉语中的词汇， 是专有名词。这些分词器默认不识别，所以需要放入扩展词典中，效果是被强制分成一个词。

## 4. 初始化IndexWriter输出流对象

IndexWriter对象用于把文件写入到磁盘里，在初始化它之前，需要配置输出的位置和config对象：

```java
// 输出路径
Directory  dir = FSDirectory.open(Paths.get("C:\\dir_lucene\\index"));
// config
IndexWriterConfig config = new IndexWriterConfig(analyzer);

// 持久化
IndexWriter indexWriter = new IndexWriter(dir, config);

for (Document doc : docList) {
    indexWriter.addDocument(doc);
}

indexWriter.close();
```



